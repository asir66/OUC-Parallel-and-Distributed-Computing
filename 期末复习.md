# 第一章：基本概念及存储结构
本章学习完需要知道：
1. 并行计算的一些概念
	1. 串行计算和并行计算具体的区别
2. 并行计算与云计算关系（并不重要）
3. 常见的计算机体系结构
4. 并行计算机存储结构
## 一些概念
串行与并行区别：
![[Pasted image 20251104161103.png]]

![[Pasted image 20251104161206.png]]

> 具体来说我认为就是
> 单计算节点 和 多计算节点 之间的区别，同一时间执行的指令数，前者为1后者为多
## 并行计算的核心基本思路
![[Pasted image 20251104161349.png]]
说白了，核心就是==分解==，要如何将任务合理的分解，分解合并之后仍然能完成任务
## 云计算与并行计算的区别

简言之，并行计算是将任务至于多个计算节点上计算。而云计算说的是计算节点在云端之上。

（我认为这里将这些并没有多大意义
## 计算机体系结构
### 从结构组成和工作方式
- 冯诺依曼体系结构
- 哈佛体系结构
[计算机的文艺复兴-冯诺依曼之外](https://www.bilibili.com/video/BV1Uf4y1o7aj/?share_source=copy_web&vd_source=3057a8c490897b7f3dc3311de6aa625f)
#### 冯诺依曼体系结构
![[Pasted image 20251104162930.png]]
#### 哈佛体系结构
哈佛结构和冯诺依曼的最大区别在于他数据和程序指令至于两个存储，并且使用两条总线执行。
### 从执行时指令数和数据流-弗林古典分类学 🌟🌟🌟
Flynn（弗林）古典分类学
- SISD：经典传统计算机
  单指令单数据流
  冯诺依曼结构
- SIMD：GPU
  单指令多数据流，即一个时钟周期，多个节点执行了同一条指令计算多条数据
- MISD：很少有这种例子，因为比较反人类
  多个指令去计算同一个数据，比较反人类
- MIMD：大型超算，典型的AMD Threadripper（县城撕裂者
  就是一个时钟周期执行多条指令，并且计算多条数据

I：指令流 （Instruction）
D：数据流

上面表示的 **单个和多个**的含义是在一个时钟周期中，CPU执行一种/多种指令流，有一个/多个数据流被计算

所以像GPU是典型的SIMD
在一个时钟周期，计算机内部同时运算同一条指令对多个数据进行运算。
#### 形象比较GPU和CPU

同时我们要注意到GPU和超多核CPU是有本质区别的。因为CPU是
![[Pasted image 20251014210614.png]]
而GPU是
![[Pasted image 20251014210637.png]]

![[Pasted image 20251014210917.png]]
所以一流的企业定标准。

### 并行计算机的存储结构 🌟🌟🌟
1. 共享型 🌟🌟🌟
	1. 共享内存-统一内存访问（UMA）
	2. 共享内存-非统一内存访问
	   缓存连贯性 🌟🌟🌟
2. 分布式内存
3. 混合型分布式内存
#### 共享内存 🌟🌟🌟
有以下特点：

![[Pasted image 20251104171308.png]]

![[a8f4f01be168935cf676ee3b48da7eac.jpg]]
共享内存设计：

UMA
![[Pasted image 20260118172842.png]]
1. SMP：（Symmetric Multiprocessing）
   我们自己的电脑宏观（不看cpu缓存）就是SMP机器
2. CC（Cache Coherence）
   缓存一致性 / 缓存连贯性
   简而言之，就是这个Cache中数据发生变化之后，其他处理器能够知道

![[Pasted image 20260118173305.png]]

#### 分布式内存
关于我不理解的点：
![[Pasted image 20251104173042.png]]

通信网络：那么难道说并行计算必须是片外甚至是机器外的互联才是吗？什么叫做全局地址空间？什么叫做缓存连贯性？我不能访问别人的Memory吗？ 🌟🌟🌟


![[Pasted image 20251104173316.png]]
此处关于并行计算机存储结构的点并没有很好的理解这点
#待完善

跳出以前的理解，本课程设计的是多机并行（分布式内存模型）
多核并行执行是共享内存模型，所以我们直观理解本课程是如何用集群实现。

# 第一点五章：HPC和架构 🌟🌟🌟
1. 什么是HPC
   HPC是高性能计算（High Performance Computing）
   通过集群架构、并行算法和相关软件以并行计算/分布式计算的方式实现单台计算机无法达到的运算速度（每秒万亿次以上）。
   HPC是一套软、硬件协同工作的系统，典型的架构包括基础设施、计算节点、存储及文件系统、网络交换、集群管理、资源调度等。
2.  主流计算架构特点和差异
	1. 指令集区别
	2. 是重核架构还是多核架构
![[Pasted image 20260118162703.png]]

杂谈：
1. 什么叫做重核架构？
   每个 CPU 核心功能很强、结构复杂、单核性能高，但核心数量相对较少的一种处理器架构。  
   每个核心内部包含了很多复杂的硬件 
   与之对比的是 众核架构 而不是多核架构
2. 什么叫做多核架构：
   多核（Multi-core）只是表示：一个芯片上有多个 CPU 核心。
# 第二章：并行评价标准

应知应会：
1. 并行机器基本性能指标
2. 加速比性能定律
	1. Amdahl定律
	2. Gustafson定律

## 基本性能指标
1. 执行时间T
   很好理解
   执行时间 = 计算时间 + 并行开销时间 + 通信时间 🌟🌟🌟
   了解的详细一点
2. 浮点运算数FLOP
   每秒浮点运算次数，为什么是浮点数，是因为浮点数往往是真实运算场景中使用的
3. 指令数目MIPS
   一种比较一般的评价指标

## 存储器性能
![[Pasted image 20251104154311.png]]

指标：
1. 带宽
   并不是说其他东西（比如容量...）不重要，但是和并行计算相关主要是这里

为什么没有查找时间呢？我认为存储查找的时间和通信开销比起来几近于无

如何估计存储器带宽呢？
![[Pasted image 20251104180142.png]]

说人话就是 **每秒操作的数据量** 

## 并行与通信开销 🌟🌟🌟🌟🌟🌟

总的来说：
1. 乒乓算法 🌟🌟🌟
	1. 一般化为热土豆算法

乒乓算法：
说人话就是测量数据从一个点到另一个点的时间。
测量方法就是发数据，对面收到之后马上返回；本人收到之后得到一个RTT，处以 2 就得到了通信开销

![[Pasted image 20251104181004.png]]
更是**simple & 蠢** 的方法：
简言之，先编号，形成环，计时数据从0到0接收。

默写下来 🌟🌟🌟
![[Pasted image 20260118163837.png]]
==代码有如若只==
### 点到点通信

### 整体通信

整体通信成本包括：
1. 播送
2. 收集
3. 散射
4. 全交换
5. 循环位移

## 加速比性能定律

首先理解为什么需要这个东西？？？

> 为了让我们能定量地预测、分析和理解并行计算的性能极限与瓶颈。
> 
> 我们将串行算法改进为并行算法，但是他的性能提升有多少？能不能理论计算得到一个极限。他的意义此时就像香农定律一样重要

简单来说是在不同情况下计算性能提升效果的“香农定律”

> [平行运算](https://zh.wikipedia.org/wiki/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97 "并行计算")中的**加速比**是用并行前的执行速度和并行后的执行速度之比来表示的，它表示了在并行化之后的效率提升情况。
> —— 百科

那么简单来说什么叫做加速比呢？
$$
S(n) = T(1) / T(n)
$$

简单来说，之前需要运行多久 / 现在加速之后需要运行多久
### Amdahl定律 —— 固定计算总量

> **阿姆达尔定律**（英语：Amdahl's law，Amdahl's argument），一个计算机科学界的[经验法则](https://zh.wikipedia.org/wiki/%E7%B6%93%E9%A9%97%E6%B3%95%E5%89%87 "经验法则")，因[吉恩·阿姆达尔](https://zh.wikipedia.org/wiki/%E5%90%89%E6%81%A9%C2%B7%E9%98%BF%E5%A7%86%E9%81%94%E7%88%BE "吉恩·阿姆达尔")而得名。它代表了[处理器](https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%A4%AE%E8%99%95%E7%90%86%E5%99%A8 "中央处理器")[并行运算](https://zh.wikipedia.org/wiki/%E4%B8%A6%E8%A1%8C%E9%81%8B%E7%AE%97 "并行运算")之后效率提升的能力。
> 
> _所以我们可以直观理解，因为这个也就是一个经验法则_

**阿姆达尔定律**是**固定负载**（计算总量不变时）时的量化标准。（所以计算量，也就是工作量不变）

ppt写的最傻逼的一集，搞不懂不说人话讲这些鸡毛
![[Pasted image 20251111141130.png]]

说人话，工作量不变的情况下，将工作分为：
1. 可并行的工作 $W_S$.   -> 给一个计算节点执行
2. 不可并行的工作 $W_P$. -> 给多个（p）计算节点执行

在后续表达中，也就是
1. 将 不可并行的工作比例设为 $f$
2. 那么可以并行的部分就是 $1-f$
3. 有额外开销另外算就好

-> 这里可以加自己的推导
### Gustafson定律 —— 可扩放问题

> 在[计算机体系结构](https://en.wikipedia.org/wiki/Computer_architecture "Computer architecture")中， **古斯塔夫森定律** （或**古斯塔夫森-巴尔西斯定律** [[ 1 ]](https://en.wikipedia.org/wiki/Gustafson%27s_law#cite_note-spp-1) ）描述了任务在[并行计算](https://en.wikipedia.org/wiki/Parallel_computing "Parallel computing")下理论上可获得的执行时间[加速比](https://en.wikipedia.org/wiki/Speedup "Speedup") ，其基准是_该任务_在单核机器上的假设运行。换句话说，它描述了_已并行化_的任务在串行机器上运行时理论上的“减速”程度。该定律以计算机科学家[约翰·L·古斯塔夫森](https://en.wikipedia.org/wiki/John_L._Gustafson "John L. Gustafson")及其同事[埃德温·H·巴尔西斯](https://en.wikipedia.org/w/index.php?title=Edwin_H._Barsis&action=edit&redlink=1 "Edwin H. Barsis (page does not exist)")的名字命名，并于 1988 年在 _《重新评估阿姆达尔定律》_ 一文中提出。

这两个定律都在回答一个问题：
> 用更多处理器（CPU）到底能让程序快多少？

简言之，Gustafson意识到，Amdahl定律会将人们指向去减少W_s，增加加速比。这个是有缺憾的，因为当计算节点增加的时候我们的计算数据也应该是增加的。只有这样才能正确反映我们的正确加速比。

==他会能要求，工作量增加，工作时间不变==
![[Pasted image 20251111144656.png]] 

简言之，我们意识到当我们的工作量与计算节点等比增加的情况下，通过第二个推导式：
$$
S' = f + p(1-f) = p + f(1-p) = p - f(p-1)
$$
如果串行占比 f 很小的话，那么当我可以用很多个节点进行计算的时候，加速比和节点数量成正比

### 来几个题目：
![[Pasted image 20260118173548.png]]

1. 什么叫做并行效率？ 
   并行效率 = 实际获得的加速比 ÷ 使用的处理器数量。 并行效率我们会用 $E$ 表示
   人话：一个处理器带来了多少的速度提升？
![[31204a9b3b776390f81f015f4f6cd757.jpg]]
## 并行计算机互联网络

- 静态互联网络
	- 一维线性阵列
	- 二维网孔
	- 树网
	- 超立方
- 动态互联网络
	- 交叉开关

关于这里可以在NoC中学习

# 第四章：并行算法设计基础

## 基础知识

并行算法分类：
1. 数值计算和非数值计算
2. 同步算法和异步算法
3. 确定算法和随机算法

![[Pasted image 20251111151137.png]]
> AI解释这一块

并行算法复杂度如何描述？
指标：
1. 运行时间T：
2. 处理器数P
3. 并行算法成本C
4. 总运算量W

### 深入学习Brent原理
> 在将一个问题的内在并行性(_inherent parallelism_)转化为一个有效的并行算法时，Brent原理（Brent’s Principle）为这种并行化提供了一个普遍的模式。[该结论最初由Richard P. Brent于1974年给出](https://dl.acm.org/doi/10.1145/321812.321815)
>
> 在这里理论的证明过程给出了转化的模式，并且该定理的结论告诉了我们这种并行算法的运行的上限。

上面是概述下面是深入讲解：

最初并行与分布式算法兴起的时候并没有合适的硬件设施供给来真正实现算法能力测量。于是之前的算法往往是依托于PRAM模型上的并行算法设计。

但是1974年Brent提出了Brent算法，在算法中提出了并行算法的上限以及在证明过程中给出了转换模式。
说人话就是==节点不够了就多执行几步==
## 并行理论模型 🌟🌟🌟
### PRAM模型 🌟🌟🌟
**PRAM（Parallel Random Access Machine）**  
全称是 **并行随机访问机模型**。

又称 SIMD-SM（SIMD-Sharing Memory）模型。

重点（考点）：🌟🌟🌟
1. 有一个共享存储器 Shared Memory
2. ==是一个隐式同步计算的方式== 他这里包含了一个隐式的同步的过程
![[Pasted image 20260118164643.png]]

> 它是一个 **理想化的并行计算机模型**，用于分析并行算法的性能和复杂度。  
> 就像 RAM（随机访问机）是串行算法的理论模型一样，PRAM 是并行算法的理想模型。

一个 PRAM 系统由：
- **若干个处理器（Processors）**：记为 P1,P2,…,PpP_1, P_2, \dots, P_pP1​,P2​,…,Pp​
- **一个全局共享内存（Shared Memory）**
- **同步控制机制**：所有处理器按“步”（synchronous steps）执行，每步执行一条简单指令。

每个处理器都可以：
- 从共享内存的任意位置读取数据； -> 所以叫做**随机访问机**模型
- 向共享内存的任意位置写入数据；
- 在本地寄存器中进行计算。

他理想化在：
1. **每个处理器速度相同；**
2. **所有处理器同步执行；**
3. **每次内存访问只需一个时间单位（O(1))；**
4. **没有通信延迟。**
👉 所以这是一个**非常理想化的模型**，主要用于分析算法的理论上限。

PRAM根据能否并发读写分为了（Concurrent并发 Exclusive互斥）
分类：
- PRAM-CRCW 可以并发读写
  其中，并发写冲突的处理规则决定了他的子类型。冲突即此时多个节点尝试对同一块内存进行读写的时候发生的冲突
	- Common（共同值写）：此时只有冲突节点写的均为同一个数据时才执行，否则发生错误。这是非常弱的，并没有冲突处理，是一种妥协
	- Arbitrary（任意的）：此时随机选择一个并写入
	- Priority（优先级）：节点间有优先级，所以比谁优先级大
	- Max/Min（规则）：简单的比大小，这个是教学模型，表示一种特殊规定
- PRAM-CREW 可以并发读，只能互斥写
- PRAM-EREW 不可并发读写

优点：
- 适合并行算法表示和复杂度分析
- 容易使用
- 隐藏通讯同步等细节

缺点：
- 不适合MIMD模型，忽略了SM的缺点（SM是理想模型大且没有竞争）

### APRAM模型
APRAM = Asynchronous PRAM（异步的PRAM）也称作MIMD-SM
意义在于补充MIMD的理想模型。

由于每个处理器都有局部的硬件，使得各处理器可以异步执行。简言之就是可以异步执行的PRAM

==显式的加入了同步路障==

![[Pasted image 20260118164841.png]]
因为异构了硬件，所以是不可以访问同一个内存单元的
### BSP模型 🌟🌟🌟
1. 超级步 🌟🌟🌟
	1. 局部计算
	2. 全局通信
	3. 路障同步

> BSP 模型是一种异步 MIMD-DM 模型，支持消息传递系统，块内异步并行，块间显式同步。BSP 模型从 PRAM 的基础上发展起来，它早期的版本就叫做XPRAM 。相比之下， APRAM 模型是一种 " 轻量级 " 同步模型，而 BSP 则是 " 大 " 同步模型。

https://blog.csdn.net/baimafujinji/article/details/51208900

上述两种模型是很好，很简单。但是其实并不实际。因为他将很多底层的东西忽略了，只能是用于理想情况下用于评价并行计算模型算法的标准。

但是用真实的机器作为模型会非常复杂，于是需要一个足够简单但是又足够准确的抽象模型，于是就有了BSP模型。

BSP 程序的执行由一系列 **Supersteps（超级步）** 构成。🌟🌟🌟
包括：
1. 本地并行计算 （局部计算）
2. 消息传递（通信） （全局通信）
3. 全局同步（barrier）（路障同步） --> 优缺点
	1. 优点：强调了计算和通讯的分离（因为计算和通讯是两个过程），提供了一个编程环境。易于程序复杂性分析
	2. 缺点：需要显式同步机制（因为最后大家一起做同步）

也就是一个化繁为简的过程，（也就是他要一个超级步一个超级步的完成，那么我们的研究对象就从整体算法到了超级步）他有三个参数：
1. p：处理器数量
2. g：通信扩张因子（gap）
3. ℓ（ell）：同步延迟（latency）

==总结性话术==
BSP模型的特点在哪里？
BSP 把一个并行程序抽象成 **一系列重复的阶段**：

> **超级步（Superstep） = 计算 → 通信 → 全局同步**

==什么叫做超级步？==
正常来说，一步可能就是一个操作。但是如果我们将步看作执行的单位的话。超级步就是表示说：
我将这一系列操作看作一步。我每执行完一步之后就会大家颗粒度对齐。

那么我们就变成了这样想：
前面先执行运算 -> 通信 -> 全局同步。这样同步之后大家又在同一线上运行。

🌟🌟🌟
![[Pasted image 20260118165156.png]]

### logP模型
> logP 模型是一种分布存储的、点到点通讯的多处理机模型，其中通讯由一组参数描述，实行隐式同步。


# 第五章：并行算法的一般设计方法

1. 串行算法的直接并行化
2. 从问题描述开始设计并行算法
   我当然知道很难，但是一点都不讲简直
3. 借用已有算法求解新问题

## 从快排讲串行算法并行化 🌟🌟🌟
简而言之，有些串行算法本身就是可以并行化的，可能是他有部分并不互相依赖，于是就可以并行化。此处从快排为例子讲解：

```cpp
#include <iostream>
#include <vector>
using namespace std;

int partition(vector<int>& arr, int low, int high) {
    int pivot = arr[high];
    int i = low - 1;

    for (int j = low; j < high; ++j) {
        if (arr[j] < pivot) {
            i++;
            swap(arr[i], arr[j]);
        }
    }

    swap(arr[i + 1], arr[high]);
    return i + 1;
}

void quicksort(vector<int>& arr, int low, int high) {
    if (low < high) {
        int p = partition(arr, low, high);

//
        quicksort(arr, low, p - 1);
        quicksort(arr, p + 1, high);
    }
}

int main() {
    vector<int> arr = {10, 7, 8, 9, 1, 5};

    quicksort(arr, 0, arr.size() - 1);

    for (int x : arr)
        cout << x << " ";
    cout << endl;
}
```
这是一个经典的快排的代码。从中我们可以看到计算核心是partition函数，并且左右部分的partition是互不关联的。于是我们可以使用BSP模型

1. 那个作为我的根？
2. 真正成功的根拿去告知所有的处理器
3. 判断左孩子 和 右孩子

🌟🌟🌟
![[Pasted image 20260118165511.png]]
简直吗的九步诗人

详细解读这个代码逻辑
看不懂，这里可能要详细解读了。

看懂了，下面是类C的代码逻辑：

```c
// 前情提要，我现在有很多个 processor，
// 我的目的是得到一颗二叉排序树
// 底层资源争夺这些我是通过硬件实现，具体实现以及选哪一个根其实一点都不重要，因为对于结果其实都一样（因为我只需要排序）
struct processor {
	int id; // 默认就是序号，后面可以直接用序号表示
	int value;
}

processor[n];

int main() {
	for each processor i do // 整颗树的根
		root = i; // 对每个节点都去抢夺根 // 谁抢到不重要，因为硬件限制，不会出现重复写这些问题，反正就是最终只有一个节点能抢到这个根节点
	end for
	
	repeat for each processor i != root do
		if (processor[i].value < now_root.value || (processor[i].value == now_root.value && i < now_root.id)) { // now_root 表示当前的根节点 || 其实就是刚才抢的根
			// 开始抢左孩子做root
		} else {
			// 开始抢右孩子做root
		}
	
}
```

解读：
1. 第一步：所有人抢占根节点，然后将左右孩子置为空
2. 第二步：所有剩下的节点开始判断自己和根谁大，然后去抢占左右孩子节点（然后变成新的根
3. 剩下的节点再重复这个过程，最后得到一棵树

优点：
1. 将互斥问题转化为了硬件竞争问题，我就不用再实现复杂的控制的东西，我直接用硬件处理（最快
2. 将顺序插入变成了全员同时插入

这是一个理想上的算法。理想情况下非常快。但是不可能现实实现
## 借用已有算法解决新问题
简而言之就是看问题的关联性，A->B
A有并行算法，所以B有并行算法（简直弱智这一块）

典型例子：
使用矩阵乘法求解点间最短路径

这里需要连接到离散数学第七章图的矩阵表示中的邻接矩阵，以及他的应用。
我们知道邻接矩阵A可以表示为$A^1$，表示一步邻接。当我是开始使用矩阵相乘运算之后我们就可以得到$A^n$表示n步相邻，也就是说从每个点走n步到另一个点的路径数量。

并且同时我可以得到可达矩阵，也就是

$$
B = A^1 + A^2 + ... + A^n
$$
并且可达为1,不可达为0

而我们这里将运算法则更换就可以得到最短路径：

那么原算法可以用数学表达式：
$$
(A×B)[i,j]=k∑​A[i,k]⋅B[k,j]
$$

而
(A⊗B)[i,j]=kmin​(A[i,k]+B[k,j])

就可以得到图中i到j的最短路径长度。

原理就是
i,k 表示i到k，k,j表示k到j。那么两者取最小得到的就是i到j。

而并行算法是已经有了成熟的并发编程算法。

# 第六章：并行算法的基本设计技术🌟🌟🌟

本章将学习 🌟🌟🌟 记住有那些（大概是填空或者简答）
1. 划分设计技术
2. 分治设计技术
3. 平衡树设计技术
4. 倍增设计技术
5. 流水线设计技术

## 划分设计技术 🌟🌟🌟

> 将给定问题分成p个独立的几乎等尺寸的子问题，用p台处理器并行处理子
问题；重在如何划分，使得子问题的解很容易被组合成原问题的解。

经典的划分方法： 🌟🌟🌟
1. 均匀划分
2. 方根划分
3. 对数划分
### 均匀划分 🌟🌟🌟

经典算法例子：
⭐PSRS算法 -- MIND 模型

有哪八步 - 简答🌟🌟🌟名字要知道，过程要会 ： 这里我应该给一个例子供学习
![[Pasted image 20251118161745.png]]

PSRS算法和相当于并发归并算法，但是他实现了将归并的过程顺利的派送给了计算节点，
1. 成功的避免了单个节点实现全局归并排序的过程
2. 也规避了多个节点数据通信的开销。

我们可以想一想如果是我们用最简单的方式是什么呢？
1. 将内容均匀划分，然后每个节点单独排序。
2. 然后大家得到了有序的例子，然后开始使用归并排序算法

but如果我们这样的话，节点间就需要比较，就需要通讯。这是非常不合理的因为通讯非常消耗时间。

而PSRS算法可以将归并排序的这个过程放到单核内执行。实现高并行性。
![[Pasted image 20260118205144.png]]
所以
1. 均匀划分：
   N=27，p=3，所以一个处理9个
2. 局部排序
   节点内部，大家可以使用任何自己想用的算法去做排序
3. 正则采样：
   节点内部均匀的选p个，这里就是3个
4. 采样排序
   将选出来的样本排列一个顺序
5. 选择主元
   选p-1个元素并且要传递给p个节点
6. 主元划分
   每个节点内部，按照给的p-1个数，将自己内部的数据划分为p个块
7. 全局交换
   将相应的块交给相应的节点。比如这里就是三个块。一个前一个中一个后。那么本质上我节点内部也是这三个块。那么也就是一个前，一个中，一个后。
8. 归并排序
   节点内部在做一次排序
### 方根划分
**1. 对 A 和 B 做方根划分（√n 分组）**
- 将 A 划成 O(√n) 块
- 将 B 划成 O(√n) 块
- 保证每块大小接近 √n，从而保证负载均衡和低通信复杂度

---
**2. 对每个 A 的块，计算其在 B 中的 rank 区间**
- 每个块 Ai（例如 Ai 的最小值和最大值）  
    去 B 中进行 rank 查询（并行二分）
- 得到 Ai 需要与 B 的哪一小段合并
- 这一步产生每个块对应的“输出区间”

**目的：让每个子任务互不重叠且连续。**

---
**3. 每个块独立、并行地做一次局部归并**
- 每个处理器负责一个块 Ai
- 与对应的 B 区间做 merge（单核即可）
- 得到一个排序好的“小段”输出

**所有归并是并行执行的。**

---

**4. 将所有已排序的小段按顺序拼接即可得到最终排序序列**
- 因为 rank 已经保证区间不重叠
- 直接连接即可得到全局有序序列

### 对数划分

对数划分也经典的使用在了归并排序并发算法中，简言之我们可以将其总结为：
1. 分任务
2. 找切点
3. 各处理器处理得到输出

他的核心思想在于==关键想法：按“输出序列”划分，而不是按“输入序列”==
所以也就是将要处理的任务均匀的分发给各个节点，然后将要处理的数位置交给他，让他处理。

那么有总共有N个数字，有P个节点，那么按理来说一个节点就应该负责输出k = N/P个数据。而且他是两个有序数组的合并，那么分了之后，我只需要满足

每个节点都完成k = N/P的输出就好了。对每个计算节点需要给他A和B的数据。所以我们需要知道我们在A和B中做划分的位置i和j，也就是切点，满足i + j = k。

而因为i + j = k，k已知，所以我们只需要确定i的位置就好了。

i 的可能范围：

`max(0, rank_p − |B|) ≤ i ≤ min(rank_p, |A|)`

这段范围长度 ≤ N。

为了快速找到 i，我们进行：

> **对 i 做二分查找（Binary Search）**

二分的时间是 O(log N)。  
这就是名字“对数划分（logarithmic partitioning）”的来源。

而需要满足
```c
A[i] >= B[j-1]
B[j] >= A[i-1]
```

找的过程使用二分查找，所以时间复杂度是log，所以叫做对数划分。

## 分治设计技术 🌟🌟🌟

双调归并网络 🌟🌟🌟
1. 双调序列 🌟🌟🌟 考定义
2. Batcher定理 考应用

分治这里主要是使用双调归并网络的例子说明一下分治技术在并行算法设计中的使用。但是具体而言，我并没有多么受到关于这个算法的归并的启发，而是发现这个算法的精妙之处。

双调序列是：
> 所谓**双调序列(Bitonic Sequence)**是指由一个非严格增序列X和非严格减序列Y（其中X的最小元素正好是Y的最大元素）构成的序列

说人话就是：
![[Pasted image 20251216144837.png]]

而 Batcher 定理是说： 🌟🌟🌟
将任意一个长为2n的双调序列A从中间切成两半，分成等长的两个序列X和Y，然后X和Y相同位置的元素xi与yi比较，小的放到Min序列，大的放到Max序列。由此得到的Max序列和Min序列也是双调序列。且Min序列的每个元素小于或等于Max序列的每个元素。

用图来看就是：
![[Pasted image 20251216145118.png]]

并且 Max 序列和 Min 序列都是双调序列。

这样的话我们就可以充分利用这个问题，因为最后的双调序列中Max中对应Min位置的数，一定是Max的大，并且这个也是一个双调序列。那么他就可以看作一个归并排序的逆过程。

在单数据的时候双调就不重要了，这时候的双调就是固定值，但是由于之前实现的Max > Min 的效果，我们就实现了一个排序的功能。

十分的NB，因为这样基于硬件的话速度可以非常快，并且适用于SIMP系统。
![[Pasted image 20260118210141.png]]

我搞不懂一点，如果我为了通用排序使用的话，我需要先将输入序列，分成两半排序。最后得到一个才可以归并序列才可以。但是不是很怪吗？
#看不懂 
## 平衡树设计技术 🌟🌟🌟

因为树本身就是非常适合与并发处理的数据结构，所以很多和树相关的算法都是可以使用并行处理。
![[Pasted image 20260118210957.png]]

这里主要学习一个算法：
计算前缀和：（注意这里的前缀和并不单指代加法，和表示的是一种运算，可以是加是乘。可以是自定义）

前缀和怎么算的 🌟🌟🌟 正向遍历 + 反向便利

![[Pasted image 20260118211010.png]]

其实就是很简单。正向便利就是最大值算法的那个思路两两求和
然后反向传递的时候就是
1. 从上到下
2. 右孩子 等于 父母节点，表示右孩子及之前的值
3. 右孩子的右边的那个的前缀和 就是 右孩子 + 他自己
## 倍增设计技术

并非很能搞得懂这里的含义，并非知道有什么意思？

## 流水线设计技术

这里介绍的是脉动阵列算法。DFT


# 第七章：并行算法的一般设计过程（PCAM）

## PCAM设计方法学

PCAM设计方法学将并行算法的设计分为了四个阶段：🌟🌟🌟 是什么，分别在干什么
1. 划分（Partitioning） 🌟🌟🌟 划分是那两种类型 就行了
	1. 域分解
	2. 功能分解
2. 通讯（Communication）
3. 组合（Agglomeration）
4. 映射（Mapping）

### 划分（partitioning）

如何划分？：
1. 充分开拓算法的并行性和可扩发性；
2. 先进行数据分解(称域分解)，再进行计算功能的分解(称功能分解)；
3. 使数据集和计算集互不相交；
4. 划分阶段忽略处理器数目和目标机器的体系结构；

其中，根据划分的依据，我们将分解具体为：
1. 域分解(domain decomposition)
2. 功能分解(functional decomposition)

划分的依据：或者说好的划分应该包括以下几个点：
1. 划分是否具有灵活性？（动态性）
2. 划分是否避免了冗余计算和存储
3. 划分后的任务负载是否大致相当
4. 任务数和问题难度是否成比例？（扩展性）

简单来说，划分的是否有
1. 可以动态调整
2. 有没有冗余
3. 是不是差不多
4. 难度 == 任务数

#### 域分解 domain decomposition

要点：
1. 划分的对象是数据，可以是算法的输入数据、中间处理数据和输出数据；
2. 将数据分解成大致相等的小数据片；
3. 划分时考虑数据上的相应操作；
4. 如果一个任务需要别的任务中的数据，则会产生任务间的通讯；

#### 功能分解 functional decomposition

功能分解并非分解功能，功能并不是这里进行划分的。这里划分的直接对象是计算，将计算划分为不同的任务。但是这个并不是目的。我们划分计算任务的目的是为了划分数据。（数据才是重要的）

因为当划分之后我们发现不同任务所需的数据是不相交的，那么我们的划分就是成功的。如果数据是有相当的重叠，那么就表示我们的划分不是非常好（不是不可以有重叠，但是重叠很多的话并行的效率就低了）
### 通讯（Communication）

四种通讯模式：
1. 局部/全局通讯
2. 结构化/非结构化通讯
3. 静态/动态
4. 同步/异步

通讯依据/好的通讯判断：
1. 所有任务是否执行大致相等的通讯？
2. 是否尽可能的局部通讯？
3. 通讯操作是否能并行执行？
4. 同步任务的计算能否并行执行？

### 组合（Agglomeration）

简而言之，组合的含义就是说：将之前分解的任务恰当的组合在一起。
因为任务组合的越散，那么真正的计算就少。但是通讯成本就高。而如果任务组合的非常紧密，那么确实是不需要通讯了，可以内部通讯而不需要外部通讯，但是这也是不好的，因为这时候有可能A部分需要一个任务，你此时要么做通讯，要么做重复运算得到相应的数据。

那么具体如何组合就是我们需要权衡的点。需要做到：
1. 通过增加任务的粒度和重复计算，可以减少通讯成本
2. 保持映射和扩展的灵活性，降低软件工程成本；

### 映射（Mapping）

组合是抽象到现实需要的过程。而如果我们最后将组合到的一坨东西交给我们真正的计算节点，做好分配。那么就完成了最后的映射阶段。

1. 每个任务需要映射到具体的处理器，定位到运行机器上
2. 任务数大于处理器数时，存在负载平衡和任务调度问题
3. 映射的目标：减少算法的执行时间

#### 负载平衡算法

1. 静态
2. 概率
3. 动态

#### 任务调度算法
1. 经理-雇员模式
2. 非集中模式
3. ...

![[Pasted image 20251222165512.png]]

集中式，很大程度肯定就是经理的通讯成本是巨大的。所以这里我们其实可以使用胖树的方式来实现会更好。

第二点：成本是很大的，因为动态负载平衡会产生很多额外的调度成本。需要
1. 监控
2. 同步与一致性
3. 调配
4. ...

# 第十二章：并行编程语言 - 并行程序设计基础

## 并行语言的构造方法：🌟🌟🌟

1. 使用库函数
	1. 获取处理器ID 和 数目
	   id = my_process_id()
	   p = number_of_processes()
	2. 库函数改串行代码
2. 拓展串行语言
3. 加编译注释构造并行程序的方法

使用库函数
- MPI：“消息传递并行编程模型”的标准接口规范
- PVM

他实现了底层通信逻辑，并且将接口和标准制定了之后给我们。我们就省去了自己实现的过程。而PVM就是已经稍微过时的，而MPI是现在主流。

串行改并行：
1. 正常该怎么写怎么写
2. 然后到需要等，停的地方写上 barrier();

这时候我们就可以来看一看我们实验的代码 🌟🌟🌟 说白了，这里就很像MPI_库编程

```c

	MPI_Init(NULL, NULL);
	// Get the number of processes
	int world_size;
	MPI_Comm_size(MPI_COMM_WORLD, &world_size);
	// Get the rank of the process
	int world_rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
	// Get the name of the processor
	char processor_name[MPI_MAX_PROCESSOR_NAME];
	int name_len;
	MPI_Get_processor_name(processor_name, &name_len);
	// Print off a hello world message
	printf("Hello world from processor %s, rank %d out of %d processors\n",
	        processor_name, world_rank, world_size);
	// Finalize the MPI environment. No more MPI calls can be made after this
	MPI_Finalize();
```

就可以知道这里的这些MPI的接口函数。于此同时，如同课程ppt中描述的一样，我们有时候是需要做进程同步的，那么这时候我们就需要设置同步屏障（Barrier）

比如我们实验二的代码中
```cpp
void treeMerge(int id, int worldSize,
               std::vector<pair<std::string, int>> &counts) {
    int participants = worldSize;
    while (participants > 1) {
        MPI_Barrier(MPI_COMM_WORLD);
        int _participants = participants / 2 + (participants % 2 ? 1 : 0);
        if (id < _participants) {
            if (id + _participants < participants) {
                std::vector<pair<std::string, int>> _counts;
                std::vector<pair<std::string, int>> temp;
                recvCounts(id + _participants, id, _counts);
                temp = mergeCounts(_counts, counts);
                counts = temp;
            }
        }
        if (id >= _participants && id < participants) {
            sendLocalCounts(id, id - _participants, counts);
        }
        participants = _participants;
    }
}
```

中的`MPI_Barrier`就是同步屏障。我们实验二中最后汇总的程序需要等待rank == 0，因为0代表根进程，而我们往往会用根进程来进行最后结算。

而我们的这种并行程序通常有以下几个步骤：
1. MPI环境初始化init
2. 获取当前有多少个计算节点
3. 看看自己是第几个计算进程
4. 并行操作
5. 结束，收尾。

![[Pasted image 20251222214043.png]]

## 交互/通信问题 🌟🌟🌟

通信的时候，同步的方式：
同步：
1. 原子同步
   设置原子操作，来同步
2. 控制同步
	1. 路障 barrer
	2. 临界区
3. 数据同步
	1. 锁 （记这个）
	2. 条件临界区
	3. 监控程序
	4. 事件

通信方式：
1. 共享变量
2. 父进程传递给子进程
3. 消息传递

按照发送方和接收方行为的区别可以分为：🌟🌟🌟
1. 一对一
	1. 点到点（Point to point）
2. 一对多
	1. 广播（Broadcast）：发送一个值给接收方全体（这个值是同一个）
	2. 播撒（scatter）：像接收方全体每一个都发送一个值
3. 多对一
	1. 收集（gather）：单纯的一个接收节点接收多个发送方的值
	2. 归约（reduce）：接收方不单单是接收，可能将数据进行计算
4. 多对多
	1. 全交换（tatal Exchange）：每个发送方向每个接收方发送不同的消息
	2. 扫描（scan）：
	3. 置换/移位（permutation / shift）：换位置

## 五种并行编程风范

- 相并行（Phase Parallel）
- 分治并行（Divide and Conquer Parallel）
- 流水线并行（Pipeline Parallel）
- 主从并行（Master-Slave Parallel）
- 工作池并行（Work Pool Parallel）
# 8.1：共享存储系统编程

本章主要是讲解三种共享存储系统编程的模型：
1. ANSI X3H5模型
2. POSIX 模型
3. OpenMP 模型

## ANSI X3H5模型

> ANSI X3H5 共享存储器模型是一个抽象的内存一致性与并行执行模型，用于规范在共享地址空间多处理器系统中，读写操作的可见性、顺序关系以及同步语义，为并行编程标准和硬件实现提供统一的理论基础。

## OpenMP模型🌟🌟🌟

说白了，就是一个更现代，更自动化，抽象层级更高的变成模型。可以通过编译指令来实现并行程序的编写。
PPT对他的定义是：🌟🌟🌟（概念是什么？）
> 一组编译制导语句和可调用的运行(run-time)库函数, 扩充到基本语言中用来表达程序中的并行性

![[Pasted image 20260117133634.png]]
这张结构图很重要

所以我们来拆解：
1. OpenMP是一组编译制导语句：所以用户实现了 Application 通过编译器就可以直接编译用到 OpenMP 的运行时库。
2. 并且他还有运行时库可以被直接调用，所以 Application 就可以直接调用相关函数
3. 用户可以通过 环境变量 来控制运行时库（也就是OpenMP运行时状态）
4. OpenMP 底层使用的是 操作系统的线程实现

编译制导语句 - Work-sharing 🌟🌟🌟
1. DO - 分配并行执行
	1. SCHEDULE 可以指定我们使用那种调度算法
2. SECTIONS  - 可以流水线执行任务
   将不同的代码块拆分出来，成为一个SECTION，然后这些代码块就可以并行的去执行了
3. SINGLE - 只有一个处理机执行
   最后，有的情况中，这个代码并不需要大家都执行。我要一个汇总的就好了。那么这时候使用SINGLE就可以让这个部分的代码由一个处理机执行。而其他的处理机会开始等待。

```cpp
解释：

上面的意思是什么呢？也就是说在OpenMP这种并行编程方式当中的意思就是说：
1. 我现在有很多个线程了，那么我应该如何去分配他们呢？
所以就提到了 Work-sharing 的方式

首先是DO，在c中叫做FOR
他会把一个循环的迭代次数，分配给多个线程并行执行。

简单来说，我下面有一个for循环，如果我使用 DO 的话，就是我的线程会各自领一个去执行。但是如果我没有写的话就是，每个线程都会都执行一遍。

并且他有 SCHEDULE 选项，可以来配置我使用什么调度算法。
```


# 8.2：分布式存储系统并行编程

而这里才是重点，主要介绍：
1. 基于消息传递的编程
2. MPI并行编程
3. 基于数据并行的并行编程

其中第二点是重点

## 基于消息传递的并行编程 🌟🌟🌟

什么叫做基于消息传递的并行编程？：

> 基于消息传递的并行编程，是一种并行计算模型：多个并行执行单元拥有各自独立的内存，通过显式发送和接收消息来交换数据与协作完成计算。
> 所以计算节点

消息传递库有很多，但是我们这里建议==MPI==
并且，课程主要讲：
1. MPI 🌟🌟🌟
2. PVM

关于消息传递，我们需要理解三个方面：
1. 总共有多少个进程
2. 进程间如何同步
3. 如何管理通信缓冲区

重点：==以及现在的消息传递系统多使用三种通信模式：== 🌟🌟🌟
1. 同步
2. 阻塞
3. 非阻塞
这张图，很重要。都要记忆 🌟🌟🌟
![[Pasted image 20251222215133.png]]

这个点就是简单的。首先
1. MPI发送方发送数据之后接收方是不会静默丢弃的。他们往往是直接要么被缓冲接收。要么阻塞等待。

这里的三个通信模式的人话是：
1. 同步：双方都要到这个节点才进行发送。所以不需要缓冲区。直接接收，不会出幺蛾子
2. 阻塞：发送方会开一个缓冲区。他不会等待接收方到了没有。而是直接发送，开一个后台直接发数据。而这里就将数据放在缓冲区。等待到消息被发完才进行下一步，也就是阻塞。
3. 非阻塞：他是不等待的，所以他开一个进程发数据。然后就直接执行。案例来说他应该wait，等待发送结束。但是不wait是不报错的。所以可能等到拷贝的时候数据就不同了。同样，接收方，知道这里需要等待一下，但是因为按理来说他是需要 wait 的，但是代码中没有，所以就有可能是他还没有收到数据就继续执行了。所以就可能出现-99
![[Pasted image 20251222221443.png]]

所以这里非阻塞模式X可能是21或者是11，也可能是-99。因为数据复制可能不及时导致后续数据发生了变化。
## MPI 并行编程 🌟🌟🌟

1. MPI中的消息
2. MPI中的消息信封
3. MPI中的四种通信模式
4. 点对点的通信群集通信
5. MPI扩展
6. 例子:计算Pi的MPI程序

把几个函数的参数搞清楚就ok了 🌟🌟🌟
考点：
1. 初始化过程 🌟🌟🌟
2. 根进程去发送消息，要么MPI
3. MPI_send MPI_recv

一个简单的例子：

```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv); // 初始化 MPI 环境

    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size); // 获得缺省的进程组大小

    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // 获得每个进程在组中的编号

    printf("Hello from rank %d of %d\n", world_rank, world_size);
    
    MPI_Send(&N, 1, MPI_INT, i, i, MPI_COMM_WORLD);
    
    MPI_Recv(&N, 1, MPI_INT, i, i, MPI_COMM_WORLD, &status);
    
    MPI_Bcast(&N, 1, MPI_INI, root, MPI_COMM_WORLD);
    
    MPI_Barrier(MPI_COMM_WORLD);

    MPI_Finalize(); // 终止 MPI 环境
    return 0;
}
```

里面 MPI_COMM_WORLD 称作通信器（通信子（人话：聊天室）），他相当于去控制这组通信。相当于一个聊天室，他规定了那些进程在一个聊天室里面。然后重新编号，就像每个单独房间有单独号码，这就够了

如何发消息和接收消息呢？🌟🌟🌟
```c
int MPI_Send(
    const void* buf,
    int count,
    MPI_Datatype datatype,
    int dest,
    int tag,
    MPI_Comm comm
);

int MPI_Recv(
    void* buf,
    int count,
    MPI_Datatype datatype,
    int source,
    int tag,
    MPI_Comm comm,
    MPI_Status* status
);
```
send

| 参数         | 类型           | 含义         |          |
| ---------- | ------------ | ---------- | -------- |
| `buf`      | 指针           | 要发送的数据地址   |          |
| `count`    | int          | 发送多少个元素    |          |
| `datatype` | MPI_Datatype | 每个元素的类型    |          |
| `dest`     | int          | 目标进程的 rank |          |
| `tag`      | int          | 消息标签       | 用来区分消息类型 |
| `comm`     | MPI_Comm     | 通信器        |          |
recv

|参数|类型|含义|
|---|---|---|
|`buf`|指针|接收数据存放位置|
|`count`|int|最多接收多少个元素|
|`datatype`|MPI_Datatype|数据类型|
|`source`|int|来源进程 rank|
|`tag`|int|匹配的消息标签|
|`comm`|MPI_Comm|通信器|
|`status`|MPI_Status*|返回实际接收信息
组成情况🌟🌟🌟，知道每一个是在干嘛
![[Pasted image 20260117151211.png]]

计算PI的例子：🌟🌟🌟

### 群集通信
群集通信方式：
1. 播撒 Scatter
   撒播也就是说，我会将这个数据发送给其他的节点
2. 聚集 Gather
   聚集，也就是别人都要将数据发给我
3. 拓展的聚集和撒播操作： Allgather
4. 全局交换 Alltoall
5. 聚合
	1. 规约
	2. 扫描
6. 路障 Barrier
   简单来说，到这里停一停，等同步。

一些详解：
![[Pasted image 20260118153000.png]]
```cpp
MPI_Scatter(SendAddress, SendCount, SendDatatype, RecvAddress, RecvCount, RecvDadatype, Root, Comm)

- SendAddress 发送方地址
- SendCount 发送的数据量
- SendDataType 发送的数据类型
- 

MPI_Gather(SendAddress, SendCount, SendDatatype, RecvAddress, RecvCount, RecvDadatype, Root, Comm)

一个广播和收集的过程
```

区别和联系：
![[Pasted image 20260118153840.png]]



1. Barrier // 路障
   MPI_Barrier(Comm) // 表示当前 Comm 中的所有进程互相同步，都要运行到这个 Barrier 的时候才会继续。不然所有的进程都会在这里等待
2. Bcast // 多播 广播
   所以说人话就是大家都会收到
   MPI_Bcast(&n, 1, MPI_LONG, 0, MPI_COMM_WORLD);
	1. &n：数据缓冲区
	2. 1：元素个数
	3. MPI_LONG：数据类型
	4. 0：root 广播源进程
	5. MRI_COMM_WORLD：通信器
3. Reduce // 加和操作
   也就是说，MPI_Reduce 用于对通信器内所有进程的数据执行归约运算（如求和、最大值、最小值、逻辑与等），并将最终结果存放到指定的 root 进程。
   MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
	1. &mypi：每个进程的输入数据
	2. &pi：root 进程接收的最终结果（计算结果放在哪里
	3. 1：元素个数
	4. MPI_DOUBLE：数据类型
	5. MPI_SUM：规约操作（因为这里不仅可以加法，还可以减法，最大最小值等等
	6. 0：根进程 根节点
	7. MPI_COMM_WORLD：通信子（频道呗

### 计算🥧

![[Pasted image 20260120133933.png]]![[Pasted image 20260120133942.png]]
![[Pasted image 20260120134000.png]]



## 课后作业
![[Pasted image 20260117151549.png]]

1. 同步的消息传递（Synchronous Message Passing）
2. 阻塞的消息传递（Blocking Message Passing）
3. 非阻塞的消息传递（Nonblocking Message Passing）

请以一个并行计算问题为例，说明MPI如何帮助解决这个问题

我认为，如果现在让我处理 1 - 10000 的加和问题，让我使用 MPI 处理。
我首先应该先知道我的计算资源有多少，然后开始尝试写程序。比如说我有10个计算节点，那么首先写程序，然后让每个计算节点计算10000分给10个，然后计算完成之后将数据都交给1号节点，然后1号节点做最后的加和。

那么最后就是简单的。
初始化，获取size，获取rank，计算，传递给1号。1号要有额外的加和操作。（但是这里的额外的加和操作我不知道怎么写）

### 消息相关概念

消息缓冲
```
buffer 消息缓冲
count 消息长度
datatype 消息类型
```

消息信封是：
```
destination 通信进程标识号
tag 消息标签
communicator 通信子
```

status的意义：
他是一个结构体，用于保存一次接收操作的返回信息。

## MPI中的四种通信模式

- 同步的（Synchronous）
- 缓冲的（Buffered）
- 标准的（Standard）
- 就绪的（Ready）

![[Pasted image 20260117163631.png]]
只有当接收方已经调用了匹配的 MPI_Recv，并且开始接收数据时，MPI_Ssend 才会返回。
所以他是可以不需要有缓冲区的

![[Pasted image 20260117163643.png]]
缓冲这里的意思也就是发送方会将数据复制到缓冲区，然后继续去做自己的事情。让一个独立的线程去发送数据。因为是复制，所以即使后面数据发生了变化缓冲区里的数据也不会变。

![[Pasted image 20260117163655.png]]
就绪的：就绪发送（Ready）要求：接收方必须已经“准备好接收”，否则行为是未定义的。
调用 MPI_Rsend 时，接收方必须已经调用了匹配的 MPI_Recv，否则行为未定义。

那么这里的含义就是说，后面的操作未定义，所以我也不会报错抛出，所以一切行为都要由程序员负责。
# 分布式一：MapReduce

简单来说
MapReduce = Map + Reduce

他的思想来源于分治的思想，让我的小弟干活，我来汇总。

所以有两个员工对象：
1. Mapper：由若干个 MapTask 组成，并行的去执行简单的任务。
2. Reducer：由若干个 ReduceTask 组成，并行的去执行汇总任务。

这张图很重要
![[Pasted image 20260118001758.png]]
1. 分片/格式化数据源
   简而言之，将要处理的源文件划分为 大小相等 的分片。然后给分片格式化为 key value 的形式。
2. 执行Map任务
   简单来说，Map 阶段开始处理 Map任务。就是前面分片的东西。
   Map 任务有100MB 的缓冲区，处理之后的中间结果会==写入缓冲区==
   缓冲区有阈值（==80MB==），如果达到阈值，系统就会将溢出数据写入磁盘。
3. 执行 Shuffle 过程
   简言之，就是处理Map阶段得到的数据如何传递给 Reduce 阶段的一个中间过程。
   分发过程中会对 数据按照 Key 分区和排序（因为一个Redece 任务只汇总 相同 key 的任务）
4. 执行Reduce任务
   简而言之，进行数据汇总计算

![[Pasted image 20260118132128.png]]
![[Pasted image 20260118132144.png]]
代码要会，代码逻辑熟悉

一个最细节的详解过程：
1. Map任务 阶段：
	1. Read
	   简单来说，读数据
	2. Map
	   简单来说，开始处理数据
	3. Collect
	   将数据暂存在一个环形缓冲区
	4. Spill
	   环形缓冲区满了之后，将数据写入到本地磁盘
	5. Combine
	   将之前得到的所有数据汇总
2. Shuffle 任务：
	1. Map 端 Shuffle 原理：
		1. 输入数据和执行 Map 任务
		2. 写入内存缓冲区
		3. 溢写（分区 排序 合并）
		4. 文件归并
	2. Reduce 端 Shuffle 过程：
		1. 领取数据
		2. 归并数据
		3. 把数据输入给Reduce 任务
3. Reduce 任务工作原理：
	1. Copy
	2. Merge
	3. Sort
	4. Reduce
	5. Write

## WordCount 计数代码实验

![[Pasted image 20260118134413.png]]

最简单伪代码
```cpp
Map(K, V) {
	For each word w in V
		Collect(w, 1);
}

Reduce(K, V[]) {
	int count = 0;
	For each v in V
		count += v;
	Collect(K, count);
}
```

接着我们来详解这部分内容：
首先
1. Map Reduce 用的是Key value 的形式。键值对的效果和意义实现。
2. 但是最开始 Map 得到的键值对过程是这样的
![[Pasted image 20260118140137.png]]
所以最开始的 Key 是文件，行号等。V 才是文本，但是是一行的文本。

所以
```cpp
For each word w in V
的含义是 V 这行文本中的 word 的集合，所以才需要 each word

然后 Collect 的过程是产生新的 <Key, Value> 的过程
这里会得到一个 <hello, 1> 的键值对
```

然后我们在 Reduce 的过程的时候，我们应该知道 Reduce 处理的时候，是处理 Key 相同的时候。那么也就是后面 Shuffle 分类给我之后我开始做一个汇总的过程：
中间的 `<Hello, 1>` 就叫做中间过程
![[Pasted image 20260118144124.png]]
所以我们再相应的去看伪代码：

```cpp
Reduce(K, V[]) {
	int count = 0;
	For each v in V
		count += v;
	Collect(K, count);
}
```

那么就是，将 三个 Hello 加和的过程。最后得到 `<Hello, 3>`

最后来详解ppt上的java伪代码：
```java
public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    // 复用同一个 IntWritable 对象，减少每个 key 输出时的对象创建与 GC 压力。
    private IntWritable result = new IntWritable();

    /**
     * 对某个 key 调用一次 reduce。
     * Hadoop 会把相同 key 的所有 value 聚合到一起，作为 values 传入。
     */
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        // 用普通 int 累加（本地计算）。
        int sum = 0;

        // values 是该 key 对应的一组计数值（例如 [1,1,1...] 或 combiner 之后的 [3,7,2...]）。
        for (IntWritable val : values) {
            // IntWritable#get() 取出 Java int 值。
            sum += val.get();
        }

        // 把求和结果写回到可序列化的 IntWritable。
        result.set(sum);

        // 输出最终结果：(word, sum)
        context.write(key, result);
    }
}

对于 value 加和加和，最后得到 sum 放回去。

public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
    // 常量 1：所有单词第一次出现都输出 1。复用对象以减少 new。
    private final static IntWritable one = new IntWritable(1);

    // 复用 Text 对象：每次 map 输出前用 set(...) 更新内容。
    private Text word = new Text();

    /**
     * Map 阶段的核心：对一条记录（通常是一行）进行处理。
     */
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // 将 Text 转为 Java 字符串，并使用 StringTokenizer 按空白字符切分。
        // 注意：该分词方式非常朴素，不会处理标点、大小写、中文分词等。
        StringTokenizer itr = new StringTokenizer(value.toString()); // 简单来说我可以将 itr 看作一个单词迭代器。

        // 对每个 token（单词）输出 (token, 1)。
        while (itr.hasMoreTokens()) { // 如果还有下一个单词 也就是 itr 所在的这个单词在这个位置上不是空的
            // 将当前 token 写入 Text 容器。
            word.set(itr.nextToken()); // 取出下一个单词 并设置到 word 变量中

            // 写出一条中间结果，后续会经过 Shuffle/Sort 按 key 聚合到 Reducer。
            context.write(word, one); // 设置 比如说 ("hello", 1)
        }
    }
}

简单来说，下面这里的逻辑就是
对于每个词，创建一个 <word, 1> 的效果
```

# 分布式二：BSP 🌟🌟🌟

> 什么叫做 负载偏斜？？？
> 说人话：因为执行超级步，有的很快执行完成了，有的要执行很久。但是是需要路障同步的，所以节点就开始盲等了。

负载偏斜
1. 引起负载偏斜的原因 -- 五大原因
	1. 负载分配不均衡
	   不按照实际情况分配，不能根据资源分任务。小马拉打车，大马拉小车
	2. 硬件配置异构
	   TPU vs CPU 算 矩阵运算当然不一样快
	3. 多用户资源争用
	   线程分的太多了，资源争夺，死锁等等
	4. 突发性中断
	   意外情况，但是可能有
	5. 逻辑算法渐进性收敛
	   也就是说：一些算法会 渐进性的收敛，说人话就是：随着时间的发展，这个算法的计算量会慢慢减小，减小
	   但是一些其他的算法其实是不会的。就导致负载的不均衡了
2. 后果： -- 三大后果
	1. 水桶效应
	   都要等运算时间最长的那个
	2. 资源利用率低
	   很多节点盲等了
	3. 设备加速老化
	   一些设备盲等，也就代表一些设备在做牛马。设备加速老化
3. 解决思路：
![[Pasted image 20260118132704.png]]
人话：

解决方案：
1 2. 合理按照资源和硬件架构分配负载
3 使用任务调度算法。不让进程出现死锁可能
3 4 5 实时监控，动态负载均衡。说人话就是说均衡一下。让空闲的去做事就好了。突发性中断就将工作交给别人
# 分布式三：HDFS 🌟🌟🌟

Hadoop Distributed FIle System 分布式文件系统

是什么？
分布式文件系统就是 把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群

简单来说 HDFS 由很多节点构成
物理结构（架构）
1. 主从结构
	1. 一个主节点（名称节点 NameNode）
		1. 管理分布式文件系统的命名空间，保存两个核心数据结构
			1. FsImage
			   维护文件系统树，以及文件树中所有的文件和文件夹的元数据
			   ==FsImage文件没有记录每个块存储在哪个数据节点==
			2. EditLog 操作日志文件
			   记录所有针对文件的创造、删除、重命名的操作
		2. 记录每个文件中各个块所在的数据节点的==位置信息==。
	2. 若干个从节点（数据节点 DataNode） -> 是在干什么
		1. 保存数据
2. Editlog文件逐渐变大，重启变慢，这时候我们应该如何做？
	1. 名称节点怎么启动？启动过程是什么样的？
		1. 名称节点启动的时候，会将 FsImage 文件中的内容加载到内存中，之后再执行 EditLog 文件中的各项操作，使得内存中的元数据和实际的同步（==人话：先画出图，然后加细节==）。
		2. 一旦启动（内存中建立了文件系统元数据的映射）之后，就创建一个新的 FsImage 和 EditLog 文件。
		3. 然后有事情写在 EditLog 文件中。
	2. 那么EditLog文件变大，我们如何处理呢？
		1. 使用第二名称节点（第二名称节点一般单独运行在一台机器上）
		2. 说人话就是：第二名称节点会定期让 名称节点先不写在 EditLog 里面，而是写在 edit.new 里面。然后由第二名称节点来将 FsImage 和 EditLog 文件合并。得到一个最新的 FsImage，然后之前的 edit.new 变成新的 EditLog

> HDFS默认一个块64MB，一个文件被分成多个块，以块作为存储单位块的大小远远大于普通文件系统，可以最小化寻址开销。

![[Pasted image 20260117193148.png]]

作为一个分布式文件管理系统，为了保证 容错性 和 可用性
冗余数据保存：
1. 多副本的方式
	1. 优点：
		1. 加快数据传输速率
		   因为我可以多点传输
		2. 容易检查数据错误
		   因为一个错不可能各个都错
		3. 保证数据可靠性
		   不可能都错

![[Pasted image 20260117194148.png]]

于是就很自然的产生了一个问题：
多副本数据怎么存放？
1. 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点
2. 第二个副本：放置在与第一个副本不同的机架的节点上
   防止同一个机架GG了，备份就完了
3. 第三个副本：与第一个副本相同机架的其他节点上
   为了更快，因为备份已经有了一个保障
4. 更多副本：随机节点
   后面就随便了
![[Pasted image 20260117212413.png]]

在运行的过程中就可能出现 数据的错误，此时就需要进行数据恢复：
数据错误和恢复
数据的出错分为：
1. 名称节点出错
   简单来说，名称节点有两个重要文件：
	1. FsImage
	2. Editlog
	   这两个文件有备份，有第二名称节点备份他们。所以如果名称节点出错了，就用第二名称节点的备份文件补充
2. 数据节点出错 --> 考点
   简而言之，HDFS 里面的数据都是有备份的。我们规定，数据节点是需要定期向名称节点发送“心跳”，也就是告诉名称节点：我还活着。然后如果 数据节点死掉了之后就发不了了。然后名称节点就会将这个数据节点标记为死，然后在其他的冗余数据节点中将数据备份一下。
3. 数据出错 --> 考点
	1. 怎么知道出错
	   简而言之，HDFS会使用MD5或者SHA-1算法计算文件数据块的Hash值，然后将Hash值放到一个文件中。读取的时候同样的方法验证 hash 值一不一样就好了
	2. 如何校验
	   我每读一个块就校验一次。如果这个块不对我就从其他的数据节点读这个数据，并且报告这个数据节点存放的是错误的，并且从正确的数据节点里得到正确的替换这里的。

---
# 关于期末考试：
第一章：
1. 弗林体系结构分类：[[Computer Architecture/并行与分布式/期末复习#从执行时指令数和数据流-弗林古典分类学 🌟🌟🌟]]

2. 三大存储结构
	1. 共享内存
	   关于 UMA 和 NUMA 的区别和特点

第二章：
1. HPC 定义
2. 主流计算架构差异和特点
3. 并行计算机基本性能指标
   执行时间包括什么
4. 并行与通信开销
	1. 乒乓算法
	   要看得懂伪代码
5. 加速比 & 加速比定律
   课后习题

第四章：
1. 并行计算模型：
	1. PRAM模型
	2. APRAM模型
	3. BSP模型
	   关于超级步
	   局部计算 全局通信 路障同步

第四章：
1. 快速排序 二叉树构造算法
   代码要会写，背都要背下来

第五章：基本设计方法
1. 有哪几种设计技术
2. 关于均匀划分 -> PSRS算法
	1. 算法步骤
	2. 步骤名称需要记住
3. 双调归并网络
4. 平衡树前缀和如何求

第六章：
1. PCAM设计方法学
   每个东西分别是什么含义
2. P 划分：
	1. 域分解
	2. 功能分解

十二章：
1. 串行并行化
	1. 库函数构造并行程序
	2. 扩展串行语言
	3. 加编译注释
2. 交互 / 通信问题
	1. 例子中的同步方式
		1. 原子同步
		2. 路障同步
		3. 临界区
		4. 数据同步
3. 交互模式
	1. 全交换
	2. 位移
	3. ...

十三章：
1. OpenMP是什么？[[Computer Architecture/并行与分布式/期末复习#OpenMP模型]]
   编译制导语句
2. 基于消息传递的并行编程
	1. 三种消息传递方式
		1. 同步
		2. 阻塞
		3. 非阻塞
	2. 三种消息传递方式可能导致的程序情况
	3. MPI 并行编程
	   简单程序要会写


第十四章：MapReduce
1. MR
	1. Map
	2. Reduce
2. 看图知道阶段
3. WordCount Java 代码的例子的 MapReduce 的过程解释，红框很重要
4. 负载偏斜
	1. 原因
	2. 后果
	3. 解决思路

十五：HDPS 重点
1. 块 是多大
2. 主从结构
3. 名称节点 -> 数据节点 -> 在干什么


https://blog.csdn.net/weixin_73895912/article/details/149357604
https://blog.csdn.net/qq_60866997/article/details/135647630

选择 x 5 - 10
填空题
判断题 x 5

简答题
关于并行计算的伪代码
上课讲的练习题

程序设计题目：
1. 基于消息传递编程 MPI
2. mapreduce 统计工作
3. 矩阵乘法（今年不考）


4. MPI_Barrier
5. MPI_Bcast

## MPI并行编程



